{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifier Template Notebook\n",
    "\n",
    "This notebook serves as a template for applying Naive Bayes classification using scikit-learn.\n",
    "It covers the standard steps:\n",
    "1. Data Loading and Initial Inspection\n",
    "2. Data Preprocessing and Train/Test Split\n",
    "3. Model Selection, Training, and Evaluation\n",
    "4. Model Tuning (if applicable for the specific Naive Bayes variant)\n",
    "\n",
    "Author: [Your Name/Pseudonym]\n",
    "Date: [Current Date]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries\n",
    "\n",
    "We start by importing all the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Scikit-learn modules\n",
    "from sklearn.datasets import load_iris # Example dataset\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') # Optional: To ignore potential warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Initial Inspection\n",
    "\n",
    "Here we load the dataset and perform some initial checks to understand its structure, features, and target variable.\n",
    "For this template, we'll use the famous Iris dataset included in scikit-learn, which is a classic example for classification.\n",
    "\n",
    "**Replace this section with your actual data loading logic if you are using a different dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "# Replace with your data loading:\n",
    "# df = pd.read_csv('your_dataset.csv')\n",
    "# Or load from other sources (databases, APIs, etc.)\n",
    "iris = load_iris()\n",
    "X = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "y = pd.Series(iris.target, name='target')\n",
    "\n",
    "# Display basic info\n",
    "print(\"Features (X) Head:\")\n",
    "print(X.head())\n",
    "print(\"\\nTarget (y) Head:\")\n",
    "print(y.head())\n",
    "\n",
    "print(\"\\nDataset Info:\")\n",
    "X.info()\n",
    "\n",
    "print(\"\\nDescriptive Statistics:\")\n",
    "print(X.describe())\n",
    "\n",
    "print(\"\\nTarget Distribution:\")\n",
    "print(y.value_counts()) # Check class balance\n",
    "\n",
    "# Optional: Visualize feature distributions (helps understand if GaussianNB is appropriate)\n",
    "# X.hist(figsize=(10, 8))\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# Optional: Pairplot to visualize relationships (can be slow on large datasets)\n",
    "# sns.pairplot(X.join(y), hue='target', palette='viridis')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing and Train/Test Split\n",
    "\n",
    "This section handles data cleaning, transformation, and splitting the data into training and testing sets.\n",
    "\n",
    "Typical steps include:\n",
    "- Handling missing values (imputation or removal)\n",
    "- Encoding categorical variables (One-Hot Encoding, Label Encoding)\n",
    "- Feature Scaling (Standardization or Normalization - **Note:** Gaussian Naive Bayes is less sensitive to scaling than distance-based models like SVMs or KNN, but other variants might benefit or require specific input types).\n",
    "- Creating new features (Feature Engineering)\n",
    "\n",
    "For the Iris dataset, these steps are minimal: no missing values, no categorical features. The primary step is splitting the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Preprocessing Steps ---\n",
    "\n",
    "# 1. Handling Missing Values (Example - uncomment if needed)\n",
    "# print(\"\\nMissing values before handling:\")\n",
    "# print(X.isnull().sum())\n",
    "# X.fillna(X.mean(), inplace=True) # Example: Impute with mean\n",
    "# print(\"\\nMissing values after handling:\")\n",
    "# print(X.isnull().sum())\n",
    "\n",
    "# 2. Encoding Categorical Variables (Example - uncomment if needed)\n",
    "# if 'categorical_column' in X.columns:\n",
    "#     X = pd.get_dummies(X, columns=['categorical_column'], drop_first=True)\n",
    "\n",
    "# 3. Feature Scaling (Optional for GaussianNB, but good practice to consider)\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# scaler = StandardScaler()\n",
    "# X_scaled = scaler.fit_transform(X)\n",
    "# X = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "# print(\"\\nFeatures after scaling (if applied):\")\n",
    "# print(X.head())\n",
    "\n",
    "\n",
    "# --- Train/Test Split ---\n",
    "# Split data into training and testing sets\n",
    "# test_size: proportion of the dataset to include in the test split\n",
    "# random_state: ensures reproducibility\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y) # stratify=y is good for classification to maintain class distribution\n",
    "\n",
    "print(f\"\\nShape of training features (X_train): {X_train.shape}\")\n",
    "print(f\"Shape of training target (y_train): {y_train.shape}\")\n",
    "print(f\"Shape of testing features (X_test): {X_test.shape}\")\n",
    "print(f\"Shape of testing target (y_test): {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Selection, Training, and Evaluation\n",
    "\n",
    "Naive Bayes is a family of probabilistic classification algorithms based on Bayes' Theorem with the \"naive\" assumption of conditional independence between features given the class.\n",
    "\n",
    "Common variants in scikit-learn:\n",
    "- `GaussianNB`: Assumes features follow a normal distribution. Suitable for continuous data.\n",
    "- `MultinomialNB`: Suitable for discrete counts. Often used for text classification (e.g., word counts).\n",
    "- `BernoulliNB`: Suitable for binary/boolean features.\n",
    "\n",
    "We'll use `GaussianNB` as the Iris dataset has continuous numerical features.\n",
    "\n",
    "This section covers:\n",
    "- Instantiating the chosen Naive Bayes model\n",
    "- Training the model on the training data\n",
    "- Evaluating the model's performance on the test data\n",
    "- Using cross-validation for a more robust performance estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Choose and Instantiate the Model ---\n",
    "\n",
    "# Select the appropriate Naive Bayes model based on your data characteristics\n",
    "# For Iris (continuous numerical data), GaussianNB is suitable.\n",
    "# For text data (word counts), use MultinomialNB.\n",
    "# For binary features, use BernoulliNB.\n",
    "\n",
    "model = GaussianNB()\n",
    "# Other options:\n",
    "# model = MultinomialNB()\n",
    "# model = BernoulliNB()\n",
    "\n",
    "\n",
    "print(f\"Selected Model: {type(model).__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Training the Model ---\n",
    "\n",
    "# Train the model using the training data\n",
    "print(\"Training the model...\")\n",
    "model.fit(X_train, y_train)\n",
    "print(\"Model training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Making Predictions ---\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Model Evaluation ---\n",
    "\n",
    "print(\"\\n--- Model Evaluation ---\")\n",
    "\n",
    "# 1. Accuracy Score: Proportion of correct predictions\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy on the test set: {accuracy:.4f}\")\n",
    "\n",
    "# 2. Confusion Matrix: Shows the counts of true positives, true negatives, false positives, and false negatives\n",
    "# Rows represent actual classes, columns represent predicted classes\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Optional: Visualize Confusion Matrix\n",
    "# plt.figure(figsize=(6, 4))\n",
    "# sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=iris.target_names, yticklabels=iris.target_names)\n",
    "# plt.xlabel('Predicted')\n",
    "# plt.ylabel('Actual')\n",
    "# plt.title('Confusion Matrix')\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# 3. Classification Report: Provides precision, recall, f1-score for each class\n",
    "# Precision: Proportion of positive identifications that were actually correct (TP / (TP + FP))\n",
    "# Recall (Sensitivity): Proportion of actual positives that were identified correctly (TP / (TP + FN))\n",
    "# F1-score: Harmonic mean of precision and recall (good for imbalanced classes)\n",
    "# Support: The number of actual occurrences of the class in the specified dataset (test set)\n",
    "class_report = classification_report(y_test, y_pred, target_names=iris.target_names)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cross-Validation ---\n",
    "# Use cross-validation for a more robust estimate of the model's performance.\n",
    "# It splits the data into 'cv' folds, trains on cv-1 folds, and tests on the remaining fold.\n",
    "# This process is repeated 'cv' times, and the results are averaged.\n",
    "\n",
    "print(\"\\n--- Cross-Validation ---\")\n",
    "cv_scores = cross_val_score(model, X, y, cv=5) # Using the whole dataset X, y here for CV\n",
    "print(f\"Cross-validation scores (5 folds): {cv_scores}\")\n",
    "print(f\"Mean cross-validation accuracy: {cv_scores.mean():.4f}\")\n",
    "print(f\"Standard deviation of cross-validation accuracy: {cv_scores.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Tuning\n",
    "\n",
    "Naive Bayes models typically have few hyperparameters compared to other algorithms.\n",
    "\n",
    "- `GaussianNB` has `var_smoothing`: a small value added to the variance for numerical stability. Tuning this might sometimes improve performance.\n",
    "- `MultinomialNB` and `BernoulliNB` have `alpha`: Laplace/Lidstone smoothing parameter. Adjusting `alpha` can help with unseen features in the test data (adds a small count to all features).\n",
    "\n",
    "We can use `GridSearchCV` to find the best value for the available hyperparameter(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Model Tuning (using GridSearchCV) ---\")\n",
    "\n",
    "# Define the parameter grid to search\n",
    "# For GaussianNB, the main parameter is var_smoothing\n",
    "param_grid = {'var_smoothing': np.logspace(0, -9, 100)} # Search across a range of values from 1e0 to 1e-9\n",
    "\n",
    "# For MultinomialNB or BernoulliNB, you might tune 'alpha'\n",
    "# param_grid = {'alpha': [0.001, 0.01, 0.1, 0.5, 1.0, 10.0]}\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "# estimator: the model to tune\n",
    "# param_grid: the grid of parameters to search\n",
    "# cv: number of cross-validation folds to use during tuning\n",
    "# scoring: metric to optimize (e.g., 'accuracy', 'f1_macro')\n",
    "grid_search = GridSearchCV(estimator=GaussianNB(), param_grid=param_grid, cv=5, scoring='accuracy') # Use GaussianNB estimator\n",
    "\n",
    "# Fit GridSearchCV to the training data (or full data X, y depending on strategy)\n",
    "# It's common practice to tune on the training set obtained *before* the final test split.\n",
    "print(\"Running GridSearchCV...\")\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"GridSearchCV complete.\")\n",
    "\n",
    "# Print the best parameters found\n",
    "print(f\"\\nBest parameters found: {grid_search.best_params_}\")\n",
    "\n",
    "# Print the best cross-validation score obtained with these parameters\n",
    "print(f\"Best cross-validation score (using training data): {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Get the best model found by GridSearchCV\n",
    "best_model = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Evaluate the best model on the held-out test set ---\n",
    "print(\"\\n--- Evaluating the best model on the test set ---\")\n",
    "\n",
    "y_pred_tuned = best_model.predict(X_test)\n",
    "\n",
    "accuracy_tuned = accuracy_score(y_test, y_pred_tuned)\n",
    "conf_matrix_tuned = confusion_matrix(y_test, y_pred_tuned)\n",
    "class_report_tuned = classification_report(y_test, y_pred_tuned, target_names=iris.target_names)\n",
    "\n",
    "\n",
    "print(f\"Accuracy on the test set (tuned model): {accuracy_tuned:.4f}\")\n",
    "print(\"\\nConfusion Matrix (tuned model):\")\n",
    "print(conf_matrix_tuned)\n",
    "print(\"\\nClassification Report (tuned model):\")\n",
    "print(class_report_tuned)\n",
    "\n",
    "# Compare with the initial model's performance on the test set\n",
    "print(f\"\\nInitial Model Test Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Tuned Model Test Accuracy:   {accuracy_tuned:.4f}\")\n",
    "# Note: For simple datasets like Iris and models like Naive Bayes, tuning might not yield significant improvements,\n",
    "# or the improvement might be slightly different between the CV score and the final test score due to data split randomness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusion and Further Steps\n",
    "\n",
    "- We have successfully loaded data, preprocessed it, trained a Naive Bayes classifier, evaluated its performance, and performed hyperparameter tuning.\n",
    "- Naive Bayes is a simple yet effective baseline model, known for its speed and interpretability (due to its probabilistic nature, although the independence assumption is a simplification).\n",
    "- Its performance can degrade if features are highly correlated, violating the independence assumption.\n",
    "\n",
    "**Further Steps:**\n",
    "- Try other Naive Bayes variants (`MultinomialNB`, `BernoulliNB`) if your data characteristics suggest it.\n",
    "- Explore other classification algorithms (Logistic Regression, SVM, Decision Trees, Random Forests, Gradient Boosting, etc.).\n",
    "- Perform more in-depth Exploratory Data Analysis (EDA).\n",
    "- Implement more sophisticated preprocessing or feature engineering techniques.\n",
    "- Handle imbalanced datasets if applicable (e.g., using techniques like SMOTE).\n",
    "- Deploy the trained model (e.g., using libraries like joblib or pickle to save the model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Saving the trained model\n",
    "# import joblib\n",
    "# filename = 'naive_bayes_model.pkl'\n",
    "# joblib.dump(best_model, filename)\n",
    "# print(f\"\\nModel saved as {filename}\")\n",
    "\n",
    "# Example: Loading the model later\n",
    "# loaded_model = joblib.load(filename)\n",
    "# print(f\"Loaded model: {loaded_model}\")\n",
    "# loaded_model.predict(X_test) # Use the loaded model"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
